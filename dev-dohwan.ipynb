{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jisoo/.conda/envs/dev/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from open_flamingo import create_model_and_transforms\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE_NUM = 0\n",
    "# SAMPLE_SIZE = 24\n",
    "# NUM_TRIALS = 1\n",
    "DATASET_NAME = 'idoll_woman'\n",
    "SORT_KEY = 'choice_rate'\n",
    "MODEL_SIZE = '9B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './'\n",
    "datasets = {}\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset00.pkl'), 'rb') as f:\n",
    "    idoll_man = pickle.load(f)\n",
    "datasets['idoll_man'] = idoll_man\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset01.pkl'), 'rb') as f:\n",
    "    idoll_woman = pickle.load(f)\n",
    "datasets['idoll_woman'] = idoll_woman\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset02.pkl'), 'rb') as f:\n",
    "    paintings = pickle.load(f)\n",
    "datasets['paintings'] = paintings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_bins(sorted_list, sample_size):\n",
    "    # Create bins from the sorted list\n",
    "    bin_size = max(1, math.ceil(len(sorted_list) / sample_size))\n",
    "    bins = [sorted_list[i:i + bin_size] for i in range(0, len(sorted_list), bin_size)]\n",
    "    return bins\n",
    "\n",
    "def sample_from_bins(bins):\n",
    "    # Randomly select one element from each bin\n",
    "    return [random.choice(bin) for bin in bins if bin]\n",
    "\n",
    "def shuffle_samples_with_indices(samples):\n",
    "    indexed_samples = list(enumerate(samples))\n",
    "    random.shuffle(indexed_samples)\n",
    "    shuffled_samples, indices = zip(*indexed_samples)\n",
    "    return list(shuffled_samples), list(indices)\n",
    "\n",
    "def sort_once_sample_shuffle_multiple_trials(tuple_list, sort_key, reverse, sample_size, trials):\n",
    "    \"\"\"\n",
    "    Example usage\n",
    "    \n",
    "    tuple_list = [\n",
    "        (1, \"http://example.com\", \"classA\", '10.00%', '15.00%'),\n",
    "        (2, \"http://example.org\", \"classB\", '5.50%', '20.00%'),\n",
    "        (3, \"http://example.net\", \"classC\", '8.75%', '12.00%'),\n",
    "    Outputs = sort_once_sample_shuffle_multiple_trials(tuple_list, 'win_rate', True, 2, 3)\n",
    "\n",
    "    Outputs: \n",
    "        [\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (1, 'http://example.com', 'classA', '10.00%', '15.00%')]),\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (1, 'http://example.com', 'classA', '10.00%', '15.00%')]),\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (3, 'http://example.net', 'classC', '8.75%', '12.00%')\n",
    "        ]\n",
    "    \"\"\"\n",
    "    if sort_key not in {'win_rate', 'choice_rate'}:\n",
    "        raise ValueError(\"sort_key must be 'win_rate' or 'choice_rate'\")\n",
    "\n",
    "    if sample_size < 1 or sample_size > len(tuple_list):\n",
    "        raise ValueError(\"sample_size must be between 1 and the length of tuple_list\")\n",
    "\n",
    "    # Function to convert percentage string to float\n",
    "    def convert_to_float(percentage_str):\n",
    "        return float(percentage_str.rstrip('%'))\n",
    "\n",
    "    # Determine the index for win_rate or choice_rate in the tuple\n",
    "    index = 3 if sort_key == 'win_rate' else 4\n",
    "\n",
    "    # Sort the list of tuples based on the specified index\n",
    "    sorted_list = sorted(tuple_list, key=lambda x: convert_to_float(x[index]), reverse=reverse)\n",
    "\n",
    "    # Create bins from the sorted list\n",
    "    bins = create_bins(sorted_list, sample_size)\n",
    "\n",
    "    results = []\n",
    "    for _ in range(trials):\n",
    "        # Sample from the bins bfor each trial\n",
    "        sample = sample_from_bins(bins)\n",
    "        shuffled_sample, original_indices = shuffle_samples_with_indices(sample)\n",
    "        results.append((shuffled_sample, original_indices))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:20<00:00,  6.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flamingo model initialized with 1384781840 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if MODEL_SIZE == '3B':\n",
    "    model, image_processor, tokenizer = create_model_and_transforms(\n",
    "        clip_vision_encoder_path=\"ViT-L-14\",\n",
    "        clip_vision_encoder_pretrained=\"openai\",\n",
    "        lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "        tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "        cross_attn_every_n_layers=1\n",
    "    )\n",
    "\n",
    "    checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "    model = model.to(DEVICE_NUM)\n",
    "elif MODEL_SIZE == '9B':\n",
    "    model, image_processor, tokenizer = create_model_and_transforms(\n",
    "        clip_vision_encoder_path=\"ViT-L-14\",\n",
    "        clip_vision_encoder_pretrained=\"openai\",\n",
    "        lang_encoder_path=\"anas-awadalla/mpt-7b\",\n",
    "        tokenizer_path=\"anas-awadalla/mpt-7b\",\n",
    "        cross_attn_every_n_layers=4\n",
    "    )\n",
    "\n",
    "    checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "    # model = model.to(DEVICE_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 33, 9, 2, 4, 11, 14, 30, 17, 18, 6, 3, 7, 25, 24, 16, 8, 32, 26, 23, 27, 29, 19, 1, 12, 5, 13, 22, 21, 10, 31, 15, 28, 20]\n",
      "34\n",
      "([0, 33, 9, 2, 4, 11, 14, 30, 17, 18, 6, 3, 7, 25, 24, 16, 8, 32, 26, 23, 27, 29, 19, 1, 12, 5, 13, 22, 21, 10, 31, 15, 28, 20], [('4', 'https://img.piku.co.kr/w/uploads/98Kx6M/3f5b7254b6bddd2707e427b4cf1721a6.jpg', '윈터(에스파)', '9.61%', '75.79%'), ('112', 'https://img.piku.co.kr/w/uploads/98Kx6M/2181c157e9c95bf4312f1e2563cb84da.jpg', '린지(픽시)', '0.12%', '13.53%'), ('43', 'https://img.piku.co.kr/w/uploads/98Kx6M/225761684daa13dd02cfa4cc409b72c1.jpg', '지우(엔믹스)', '0.78%', '54.73%'), ('7', 'https://img.piku.co.kr/w/uploads/98Kx6M/20448f9895913595923d65c152eb0f4b.jpg', '이서(아이브)', '7.08%', '71.71%'), ('11', 'https://img.piku.co.kr/w/uploads/98Kx6M/e2a8e01c4950c7784c5ec0556f18a158.jpg', '리즈(아이브)', '5.80%', '67.34%'), ('53', 'https://img.piku.co.kr/w/uploads/98Kx6M/572d6481f924143c4065a96ebda6a88b.jpg', '신비(비비지)', '0.57%', '49.16%'), ('57', 'https://img.piku.co.kr/w/uploads/98Kx6M/08440b906d9a76b071ff45dfdb140108.jpg', '두나(첫사랑)', '0.49%', '44.82%'), ('124', 'https://img.piku.co.kr/w/uploads/98Kx6M/8a8f7453e92ec272d9d2348a28f6bba2.jpg', '온다(에버글로우)', '0.03%', '23.63%'), ('94', 'https://img.piku.co.kr/w/uploads/98Kx6M/e0f91257e8f51ada241849f00aee5077.jpg', '서연(첫사랑)', '0.16%', '41.35%'), ('81', 'https://img.piku.co.kr/w/uploads/98Kx6M/81d9132b068413bc9f5beb29539247b1.jpg', '지한(위클리)', '0.26%', '41.27%'), ('17', 'https://img.piku.co.kr/w/uploads/98Kx6M/9a1fa239addd6be4f1f076def0fb4a72.jpg', '레이(아이브)', '3.43%', '59.55%'), ('19', 'https://img.piku.co.kr/w/uploads/98Kx6M/2502084740b7acff48423208bff9c57a.jpg', '유나(있지)', '3.26%', '67.65%'), ('30', 'https://img.piku.co.kr/w/uploads/98Kx6M/f25efd93e70dbddc49453c2265beb0d2.jpg', '혜인(뉴진스)', '1.38%', '59.22%'), ('102', 'https://img.piku.co.kr/w/uploads/98Kx6M/19e297b0c3021f61c20b3afced2a68a1.jpg', '로라(픽시)', '0.14%', '31.92%'), ('101', 'https://img.piku.co.kr/w/uploads/98Kx6M/08856070cd8b0fd18b2b0cc3aadd027d.jpg', '채린(체리블렛)', '0.14%', '35.04%'), ('78', 'https://img.piku.co.kr/w/uploads/98Kx6M/037011fc4d6422bfc64664693ed59fa6.jpg', '서영은(케플러)', '0.28%', '43.53%'), ('37', 'https://img.piku.co.kr/w/uploads/98Kx6M/27e37362da5e24374b1e02c025bb6df2.jpg', '류진(있지)', '0.94%', '55.87%'), ('131', 'https://img.piku.co.kr/w/uploads/98Kx6M/169a7a83230c76565a21ada802c0defc.jpg', '박소은(위클리)', '0.02%', '18.75%'), ('92', 'https://img.piku.co.kr/w/uploads/98Kx6M/954e66a0bdfb3cfaebb9e2c02e2a6ad0.jpg', '예함(첫사랑)', '0.17%', '31.41%'), ('109', 'https://img.piku.co.kr/w/uploads/98Kx6M/0193a6dc85cfd49c67fbc65423f5d5c5.jpg', '윤경(로켓펀치)', '0.09%', '35.43%'), ('116', 'https://img.piku.co.kr/w/uploads/98Kx6M/a26299dc46c2d9fb5870748587121cd8.jpg', '휘서(하이키)', '0.06%', '29.94%'), ('119', 'https://img.piku.co.kr/w/uploads/98Kx6M/fb087d8aed61a0050d10e1d5af75d115.jpg', '해윤(체리블렛)', '0.06%', '26.73%'), ('106', 'https://img.piku.co.kr/w/uploads/98Kx6M/6f8b4ff70523974d9585a2bd686ffc00.jpg', '김다연(케플러)', '0.11%', '40.25%'), ('8', 'https://img.piku.co.kr/w/uploads/98Kx6M/0b994ec0d99aeb44c23cc15f1d64e1c5.jpg', '하니(뉴진스)', '6.81%', '72.32%'), ('42', 'https://img.piku.co.kr/w/uploads/98Kx6M/5b2157fb9a534a0b982e216ace0c4634.jpg', '시현(첫사랑)', '0.81%', '47.45%'), ('27', 'https://img.piku.co.kr/w/uploads/98Kx6M/14b5e96cf5d48ad6ec65a5febc87b442.jpg', '최예나', '1.74%', '62.43%'), ('40', 'https://img.piku.co.kr/w/uploads/98Kx6M/44261f3bd6545853dc3c3310e770cb3a.jpg', '수아(첫사랑)', '0.86%', '46.82%'), ('88', 'https://img.piku.co.kr/w/uploads/98Kx6M/6ef0aa9c3c9f35192259a1ec6330264e.jpg', '히나(라잇썸)', '0.18%', '37.62%'), ('99', 'https://img.piku.co.kr/w/uploads/98Kx6M/59f7b9e7e0f4b5959a9d9db9fbbd9aab.jpg', '지원(체리블렛)', '0.14%', '38.58%'), ('46', 'https://img.piku.co.kr/w/uploads/98Kx6M/c3ab0ee67a5b1875bfe77ef4065b0f01.jpg', '강예서(케플러)', '0.72%', '52.89%'), ('125', 'https://img.piku.co.kr/w/uploads/98Kx6M/6bf8465ee910c5e7665169ba51081883.jpg', '미아(에버글로우)', '0.03%', '23.28%'), ('67', 'https://img.piku.co.kr/w/uploads/98Kx6M/e6e4a2c6d14fcf4bfe29f4128ef804ac.jpg', '명형서(클라씨)', '0.40%', '43.64%'), ('93', 'https://img.piku.co.kr/w/uploads/98Kx6M/cf87d84af96aa84e1f075d3de00de77d.jpg', '아샤(에버글로우)', '0.17%', '28.49%'), ('95', 'https://img.piku.co.kr/w/uploads/98Kx6M/c607bd98514555b4ef93f38460bc9441.jpg', '리아(있지)', '0.15%', '38.98%')])\n"
     ]
    }
   ],
   "source": [
    "NUM_CONTEXT = 35 # (2 * 24)\n",
    "NUM_QUERY = 7 # ( 2 * 4)\n",
    "\n",
    "sampled_results = sort_once_sample_shuffle_multiple_trials(datasets[DATASET_NAME], sort_key=SORT_KEY, reverse=True, \n",
    "                                                       sample_size=(NUM_CONTEXT + NUM_QUERY), trials=1)\n",
    "print(sampled_results[0][0])\n",
    "print(len(sampled_results[0][0]))\n",
    "print(sampled_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "torch.Size([30, 3, 224, 224])\n",
      "Context torch.Size([1, 30, 3, 224, 224]) \n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "torch.Size([4, 3, 224, 224])\n",
      "Query torch.Size([1, 4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# sampled_results Length: NUM_TRIALS, SAMPLES: NUM_SAMPLES\n",
    "NUM_CONTEXT = 30 \n",
    "NUM_QUERY = 4\n",
    "\n",
    "context_scores = {}\n",
    "query_scores = {}\n",
    "\n",
    "vision_context = []\n",
    "for trial in sampled_results:\n",
    "    samples = []\n",
    "    for i, sample in enumerate(trial[1][0:NUM_CONTEXT]):\n",
    "        print(i)\n",
    "        x = image_processor(Image.open(\n",
    "            f'./Dataset01/Data01_{sample[0]}.jpg'\n",
    "        ))\n",
    "        samples.append(x)\n",
    "        context_scores[i] = sample[4]\n",
    "        \n",
    "    samples = torch.stack(samples, dim=0)\n",
    "    print(samples.shape)\n",
    "    vision_context.append(samples)\n",
    "\n",
    "vision_context = torch.stack(vision_context, dim=0)\n",
    "print(\"Context\", vision_context.shape, \"\\n\")\n",
    "\n",
    "\n",
    "vision_query = []\n",
    "for trial in sampled_results:\n",
    "    samples = []\n",
    "    for i, sample in enumerate(trial[1][NUM_CONTEXT:NUM_CONTEXT + NUM_QUERY + 1]):\n",
    "        print(i)\n",
    "        x = image_processor(Image.open(\n",
    "            f'./Dataset01/Data01_{sample[0]}.jpg'\n",
    "        ))\n",
    "        query_scores[i] = sample[4]\n",
    "        samples.append(x)\n",
    "    samples = torch.stack(samples, dim=0)\n",
    "    print(samples.shape)\n",
    "    vision_query.append(samples)\n",
    "vision_query = torch.stack(vision_query, dim=0)\n",
    "print(\"Query\", vision_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '75.79%',\n",
       " 1: '13.53%',\n",
       " 2: '54.73%',\n",
       " 3: '71.71%',\n",
       " 4: '67.34%',\n",
       " 5: '49.16%',\n",
       " 6: '44.82%',\n",
       " 7: '23.63%',\n",
       " 8: '41.35%',\n",
       " 9: '41.27%',\n",
       " 10: '59.55%',\n",
       " 11: '67.65%',\n",
       " 12: '59.22%',\n",
       " 13: '31.92%',\n",
       " 14: '35.04%',\n",
       " 15: '43.53%',\n",
       " 16: '55.87%',\n",
       " 17: '18.75%',\n",
       " 18: '31.41%',\n",
       " 19: '35.43%',\n",
       " 20: '29.94%',\n",
       " 21: '26.73%',\n",
       " 22: '40.25%',\n",
       " 23: '72.32%',\n",
       " 24: '47.45%',\n",
       " 25: '62.43%',\n",
       " 26: '46.82%',\n",
       " 27: '37.62%',\n",
       " 28: '38.58%',\n",
       " 29: '52.89%'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '23.28%', 1: '43.64%', 2: '28.49%', 3: '38.98%'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['75.79%', '13.53%']\n",
      "['54.73%', '71.71%']\n",
      "['67.34%', '49.16%']\n",
      "['44.82%', '23.63%']\n",
      "['41.35%', '41.27%']\n",
      "['59.55%', '67.65%']\n",
      "['59.22%', '31.92%']\n",
      "['35.04%', '43.53%']\n",
      "['55.87%', '18.75%']\n",
      "['31.41%', '35.43%']\n",
      "['29.94%', '26.73%']\n",
      "['40.25%', '72.32%']\n",
      "['47.45%', '62.43%']\n",
      "['46.82%', '37.62%']\n",
      "['38.58%', '52.89%']\n",
      "['(A)', '(B)', '(A)', '(A)', '(A)', '(B)', '(A)', '(B)', '(A)', '(B)', '(A)', '(B)', '(B)', '(A)', '(B)']\n"
     ]
    }
   ],
   "source": [
    "context_answer = []\n",
    "len_c = len(context_scores)\n",
    "\n",
    "for i in range(len_c // 2):\n",
    "    curr_ = list(context_scores.values())[i * 2: i * 2 + 2]\n",
    "    answer = '(B)'\n",
    "    if curr_[0] > curr_[1]:\n",
    "        answer = '(A)'\n",
    "    context_answer.append(answer)\n",
    "        \n",
    "    print(curr_)\n",
    "print(context_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['23.28%', '43.64%']\n",
      "['28.49%', '38.98%']\n",
      "['(B)', '(B)']\n"
     ]
    }
   ],
   "source": [
    "query_answer = []\n",
    "len_q = len(query_scores)\n",
    "\n",
    "for i in range(len_q // 2):\n",
    "    curr_ = list(query_scores.values())[i * 2: i * 2 + 2]\n",
    "    answer = '(B)'\n",
    "    if curr_[0] > curr_[1]:\n",
    "        answer = '(A)'\n",
    "    query_answer.append(answer)\n",
    "        \n",
    "    print(curr_)\n",
    "print(query_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_score_answer(scores):\n",
    "    curr_answer = []\n",
    "    len_ = len(scores)\n",
    "    \n",
    "    for i in range(len_ // 2):\n",
    "        curr_ = list(scores.values())[i * 2: i * 2 + 2]\n",
    "        answer = '(B)'\n",
    "        if curr_[0] > curr_[1]:\n",
    "            answer = '(A)'\n",
    "        curr_answer.append(answer)\n",
    "            \n",
    "        print(curr_)\n",
    "    print(curr_answer)\n",
    "    return curr_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|> \n",
      "\n",
      "Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer:\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 2\n",
    "task_prompt_text = ''\n",
    "question_prompt = 'Choose the more aesthetic image. Options:'\n",
    "len_c = len(context_scores)\n",
    "len_q = len(query_scores)\n",
    "\n",
    "\n",
    "context_text = task_prompt_text + ''.join([\n",
    "    question_prompt + f\"(A) <image> \b(B) <image>. Answer: {context_answer[i]} <|endofchunk|>\"\n",
    "    for i in range(len_c // 2)\n",
    "])\n",
    "context_text = context_text + f\"<|endofchunk|>\"\n",
    "print(context_text, \"\\n\")\n",
    "\n",
    "queries_text = [\n",
    "    question_prompt + f\"(A) <image> \b(B) <image>. Answer:\"\n",
    "    for _ in range(NUM_TRIALS)\n",
    "]\n",
    "\n",
    "print(queries_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 30, 1, 3, 224, 224]), torch.Size([1, 4, 1, 3, 224, 224]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_context[None, 0, :].unsqueeze(2)[:, :, :].shape, vision_query[:, :, :].unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_QUERY // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(vision_context, vision_query, context_text, queries_text, query_answer, num):\n",
    "    print(f\"Query answer: {query_answer}\")\n",
    "\n",
    "    \"\"\"\n",
    "    Step 4: Generate text\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        context_len = len(context_text)\n",
    "        for q_i in range(NUM_QUERY // 2):\n",
    "            start = time.time()\n",
    "            print(f\"Start time: {start}\")\n",
    "            vision_x = torch.concat([vision_context[None, 0, :].unsqueeze(2)[:, :num, :],  vision_query[:, q_i * 2: q_i * 2 + 2, :].unsqueeze(2)], dim=1)\n",
    "            lang_x = tokenizer(\n",
    "                [context_text + queries_text[0] for _ in range(1)], \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            generated_text = model.generate(\n",
    "                vision_x=vision_x,\n",
    "                lang_x=lang_x[\"input_ids\"],\n",
    "                attention_mask=lang_x[\"attention_mask\"],\n",
    "                max_new_tokens=3,\n",
    "                num_beams=3,\n",
    "            )\n",
    "    \n",
    "            generated = tokenizer.decode(generated_text[0])\n",
    "            print(\"Context: \", generated[:context_len])\n",
    "            print(\"Generated text: \", generated[context_len:])\n",
    "            end = time.time()\n",
    "            print(f\"Time {end - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 4, 3, 224, 224])\n",
      "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "white_ = torch.ones([3, 224, 224])\n",
    "print(white_.shape)\n",
    "# Original Query \n",
    "# ['60.53%', '57.23%']\n",
    "# ['38.21%', '22.05%']\n",
    "# ['28.60%', '54.87%']\n",
    "# ['(A)', '(A)', '(B)']\n",
    "\n",
    "vision_query_new = vision_query\n",
    "choosen = [0, 2] \n",
    "for i in choosen:\n",
    "    vision_query_new[0, i, :] = white_\n",
    "print(vision_query_new.shape)\n",
    "print(vision_query_new[0, 0])\n",
    "vision_query_new.shape\n",
    "new_answer = ['(B)', '(B)']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query answer: ['(B)', '(B)', '(A)']\n",
      "Start time: 1701733281.9379327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jisoo/.conda/envs/dev/lib/python3.8/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 139.6596667766571\n",
      "Start time: 1701733421.597614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 137.41010999679565\n"
     ]
    }
   ],
   "source": [
    " predict(vision_context, vision_query_new, context_text, queries_text, new_answer, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query answer: ['(B)', '(B)']\n",
      "Start time: 1701733559.013797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 135.19820618629456\n",
      "Start time: 1701733694.2120147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 135.71186876296997\n"
     ]
    }
   ],
   "source": [
    " predict(vision_context, vision_query, context_text, queries_text, query_answer, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 48, 3, 224, 224])\n",
      "0\n",
      "3\n",
      "5\n",
      "6\n",
      "8\n",
      "10\n",
      "12\n",
      "15\n",
      "16\n",
      "18\n",
      "21\n",
      "22\n",
      "25\n",
      "26\n",
      "28\n",
      "31\n",
      "33\n",
      "34\n",
      "37\n",
      "39\n",
      "41\n",
      "42\n",
      "44\n",
      "46\n",
      "['0.0%', '45.61%']\n",
      "['52.35%', '0.0%']\n",
      "['47.87%', '0.0%']\n",
      "['0.0%', '41.55%']\n",
      "['0.0%', '77.56%']\n",
      "['0.0%', '57.58%']\n",
      "['0.0%', '49.43%']\n",
      "['59.95%', '0.0%']\n",
      "['0.0%', '32.98%']\n",
      "['0.0%', '30.75%']\n",
      "['57.93%', '0.0%']\n",
      "['0.0%', '53.77%']\n",
      "['42.15%', '0.0%']\n",
      "['0.0%', '42.53%']\n",
      "['0.0%', '24.36%']\n",
      "['64.76%', '0.0%']\n",
      "['49.81%', '0.0%']\n",
      "['0.0%', '45.39%']\n",
      "['69.69%', '0.0%']\n",
      "['44.85%', '0.0%']\n",
      "['61.76%', '0.0%']\n",
      "['0.0%', '42.84%']\n",
      "['0.0%', '62.68%']\n",
      "['0.0%', '50.37%']\n",
      "['(B)', '(A)', '(A)', '(B)', '(B)', '(B)', '(B)', '(A)', '(B)', '(B)', '(A)', '(B)', '(A)', '(B)', '(B)', '(A)', '(A)', '(B)', '(A)', '(A)', '(A)', '(B)', '(B)', '(B)']\n"
     ]
    }
   ],
   "source": [
    "white_ = torch.ones([3, 224, 224])\n",
    "print(white_.shape)\n",
    "# Original Query \n",
    "# ['60.53%', '57.23%']\n",
    "# ['38.21%', '22.05%']\n",
    "# ['28.60%', '54.87%']\n",
    "# ['(A)', '(A)', '(B)']\n",
    "\n",
    "new_answer = []\n",
    "vision_context_new = vision_context\n",
    "context_scores_new = context_scores\n",
    "print(vision_context_new.shape)\n",
    "for i in range(vision_context.shape[1]// 2):\n",
    "    curr = random.choice([i*2, i*2 + 1])\n",
    "    vision_context_new[0, curr, :] = white_\n",
    "    context_scores_new[curr] = '0.0%'\n",
    "    print(curr)\n",
    "\n",
    "context_answer_new = []\n",
    "len_c = len(context_scores_new)\n",
    "\n",
    "for i in range(len_c // 2):\n",
    "    curr_ = list(context_scores_new.values())[i * 2: i * 2 + 2]\n",
    "    answer = '(B)'\n",
    "    if curr_[0] > curr_[1]:\n",
    "        answer = '(A)'\n",
    "    context_answer_new.append(answer)\n",
    "        \n",
    "    print(curr_)\n",
    "print(context_answer_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|> \n",
      "\n",
      "Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer:\n"
     ]
    }
   ],
   "source": [
    "task_prompt_text = ''\n",
    "question_prompt = 'Choose the more aesthetic image. Options:'\n",
    "len_c = len(context_scores)\n",
    "len_q = len(query_scores)\n",
    "\n",
    "\n",
    "context_text = task_prompt_text + ''.join([\n",
    "    question_prompt + f\"(A) <image> \b(B) <image>. Answer: {context_answer_new[i]} <|endofchunk|>\"\n",
    "    for i in range(len_c // 2)\n",
    "])\n",
    "context_text = context_text + f\"<|endofchunk|>\"\n",
    "print(context_text, \"\\n\")\n",
    "\n",
    "queries_text = [\n",
    "    question_prompt + f\"(A) <image> \b(B) <image>. Answer:\"\n",
    "    for _ in range(NUM_TRIALS)\n",
    "]\n",
    "\n",
    "print(queries_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query answer: ['(B)', '(B)', '(A)']\n",
      "Start time: 1701693849.3406339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 226.13598132133484\n",
      "Start time: 1701694075.4766266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 223.60786080360413\n",
      "Start time: 1701694299.0844991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 223.17199873924255\n"
     ]
    }
   ],
   "source": [
    " predict(vision_context_new, vision_query_new, context_text, queries_text, new_answer, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score evaluated based on the final rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "torch.Size([48, 3, 224, 224])\n",
      "Context torch.Size([1, 48, 3, 224, 224]) \n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "torch.Size([7, 3, 224, 224])\n",
      "Query torch.Size([1, 7, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "3\n",
    "\n",
    "# sampled_results Length: NUM_TRIALS, SAMPLES: NUM_SAMPLES\n",
    "context_scores = {}\n",
    "query_scores = {}\n",
    "\n",
    "vision_context = []\n",
    "for trial in sampled_results:\n",
    "    samples = []\n",
    "    for i, sample in enumerate(trial[1][0:NUM_CONTEXT]):\n",
    "        print(i)\n",
    "        x = image_processor(Image.open(\n",
    "            f'./Dataset00/Data00_{sample[0]}.jpg'\n",
    "        ))\n",
    "        samples.append(x)\n",
    "        context_scores[i] = sample[3]\n",
    "        \n",
    "    samples = torch.stack(samples, dim=0)\n",
    "    print(samples.shape)\n",
    "    vision_context.append(samples)\n",
    "\n",
    "vision_context = torch.stack(vision_context, dim=0)\n",
    "print(\"Context\", vision_context.shape, \"\\n\")\n",
    "\n",
    "\n",
    "vision_query = []\n",
    "for trial in sampled_results:\n",
    "    samples = []\n",
    "    for i, sample in enumerate(trial[1][NUM_CONTEXT:NUM_CONTEXT + NUM_QUERY + 1]):\n",
    "        print(i)\n",
    "        x = image_processor(Image.open(\n",
    "            f'./Dataset00/Data00_{sample[0]}.jpg'\n",
    "        ))\n",
    "        query_scores[i] = sample[3]\n",
    "        samples.append(x)\n",
    "    samples = torch.stack(samples, dim=0)\n",
    "    print(samples.shape)\n",
    "    vision_query.append(samples)\n",
    "vision_query = torch.stack(vision_query, dim=0)\n",
    "print(\"Query\", vision_query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.38%', '0.98%']\n",
      "['0.35%', '0.04%']\n",
      "['0.47%', '0.45%']\n",
      "['0.13%', '0.30%']\n",
      "['0.02%', '11.31%']\n",
      "['1.13%', '1.86%']\n",
      "['0.06%', '0.50%']\n",
      "['1.36%', '0.07%']\n",
      "['0.19%', '0.11%']\n",
      "['0.34%', '0.15%']\n",
      "['1.21%', '4.51%']\n",
      "['3.68%', '1.07%']\n",
      "['0.11%', '2.75%']\n",
      "['0.04%', '0.31%']\n",
      "['0.15%', '0.06%']\n",
      "['3.65%', '0.15%']\n",
      "['0.65%', '2.15%']\n",
      "['0.02%', '0.08%']\n",
      "['4.20%', '0.04%']\n",
      "['0.21%', '0.22%']\n",
      "['2.93%', '3.17%']\n",
      "['0.24%', '0.13%']\n",
      "['0.21%', '2.28%']\n",
      "['0.11%', '1.06%']\n",
      "['(B)', '(A)', '(A)', '(B)', '(B)', '(B)', '(B)', '(A)', '(A)', '(A)', '(B)', '(A)', '(B)', '(B)', '(A)', '(A)', '(B)', '(B)', '(A)', '(B)', '(B)', '(A)', '(B)', '(B)']\n",
      "['1.29%', '1.38%']\n",
      "['0.30%', '0.02%']\n",
      "['0.23%', '1.42%']\n",
      "['(B)', '(A)', '(B)']\n"
     ]
    }
   ],
   "source": [
    "context_answer = []\n",
    "len_c = len(context_scores)\n",
    "\n",
    "for i in range(len_c // 2):\n",
    "    curr_ = list(context_scores.values())[i * 2: i * 2 + 2]\n",
    "    answer = '(B)'\n",
    "    if curr_[0] > curr_[1]:\n",
    "        answer = '(A)'\n",
    "    context_answer.append(answer)\n",
    "        \n",
    "    print(curr_)\n",
    "print(context_answer)\n",
    "\n",
    "query_answer = []\n",
    "len_q = len(query_scores)\n",
    "\n",
    "for i in range(len_q // 2):\n",
    "    curr_ = list(query_scores.values())[i * 2: i * 2 + 2]\n",
    "    answer = '(B)'\n",
    "    if curr_[0] > curr_[1]:\n",
    "        answer = '(A)'\n",
    "    query_answer.append(answer)\n",
    "        \n",
    "    print(curr_)\n",
    "print(query_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|> \n",
      "\n",
      "Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer:\n"
     ]
    }
   ],
   "source": [
    "task_prompt_text = ''\n",
    "question_prompt = 'Choose the more aesthetic image. Options:'\n",
    "len_c = len(context_scores)\n",
    "len_q = len(query_scores)\n",
    "\n",
    "\n",
    "context_text = task_prompt_text + ''.join([\n",
    "    question_prompt + f\"(A) <image> \b(B) <image>. Answer: {context_answer[i]} <|endofchunk|>\"\n",
    "    for i in range(len_c // 2)\n",
    "])\n",
    "context_text = context_text + f\"<|endofchunk|>\"\n",
    "print(context_text, \"\\n\")\n",
    "\n",
    "queries_text = [\n",
    "    question_prompt + f\"(A) <image> \b(B) <image>. Answer:\"\n",
    "    for _ in range(NUM_TRIALS)\n",
    "]\n",
    "\n",
    "print(queries_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query answer: ['(B)', '(A)', '(B)']\n",
      "Start time: 1701694527.374507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 223.96712589263916\n",
      "Start time: 1701694751.3416445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 224.85135984420776\n",
      "Start time: 1701694976.1930158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Choose the more aesthetic image. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 223.91530203819275\n"
     ]
    }
   ],
   "source": [
    " predict(vision_context, vision_query, context_text, queries_text, query_answer, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|> \n",
      "\n",
      "Which image is better?. Options:(A) <image>(B) <image>. Answer:\n"
     ]
    }
   ],
   "source": [
    "task_prompt_text = ''\n",
    "question_prompt = 'Which image is better?. Options:'\n",
    "len_c = len(context_scores)\n",
    "len_q = len(query_scores)\n",
    "\n",
    "\n",
    "context_text = task_prompt_text + ''.join([\n",
    "    question_prompt + f\"(A) <image> \b(B) <image>. Answer: {context_answer[i]} <|endofchunk|>\"\n",
    "    for i in range(len_c // 2)\n",
    "])\n",
    "context_text = context_text + f\"<|endofchunk|>\"\n",
    "print(context_text, \"\\n\")\n",
    "\n",
    "queries_text = [\n",
    "    question_prompt + f\"(A) <image> \b(B) <image>. Answer:\"\n",
    "    for _ in range(NUM_TRIALS)\n",
    "]\n",
    "\n",
    "print(queries_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query answer: ['(B)', '(A)', '(B)']\n",
      "Start time: 1701695200.1250484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Which image is better?. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 188.1906177997589\n",
      "Start time: 1701695388.315679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (A) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|>Which image is better?. Options:(A) <image>(B) <image>. Answer: (B) <|endofchunk|><|endofchunk|>\n",
      "Generated text:  Which image is better?. Options:(A) <image>(B) <image>. Answer: (A)\n",
      "Time 193.49916195869446\n",
      "Start time: 1701695581.8148534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    " predict(vision_context, vision_query, context_text, queries_text, query_answer, 48)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
