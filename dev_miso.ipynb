{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "os.chdir('/home/miso/agi/open_flamingo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argument\n",
    "MODEL_SIZE = '9B'\n",
    "DEVICE_NUM = 7\n",
    "SAMPLE_SIZE = 6\n",
    "NUM_TRIALS = 5\n",
    "DATASET_NAME = 'idoll_man'\n",
    "SORT_KEY = 'choice_rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "root_dir = '../'\n",
    "datasets = {}\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset00.pkl'), 'rb') as f:\n",
    "    idoll_man = pickle.load(f)\n",
    "datasets['idoll_man'] = idoll_man\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset01.pkl'), 'rb') as f:\n",
    "    idoll_woman = pickle.load(f)\n",
    "datasets['idoll_woman'] = idoll_woman\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset02.pkl'), 'rb') as f:\n",
    "    paintings = pickle.load(f)\n",
    "datasets['paintings'] = paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def create_bins(sorted_list, sample_size):\n",
    "    # Create bins from the sorted list\n",
    "    bin_size = max(1, math.ceil(len(sorted_list) / sample_size))\n",
    "    bins = [sorted_list[i:i + bin_size] for i in range(0, len(sorted_list), bin_size)]\n",
    "    return bins\n",
    "\n",
    "def sample_from_bins(bins):\n",
    "    # Randomly select one element from each bin\n",
    "    return [random.choice(bin) for bin in bins if bin]\n",
    "\n",
    "def shuffle_samples_with_indices(samples):\n",
    "    indexed_samples = list(enumerate(samples))\n",
    "    random.shuffle(indexed_samples)\n",
    "    shuffled_samples, indices = zip(*indexed_samples)\n",
    "    return list(shuffled_samples), list(indices)\n",
    "\n",
    "def sort_once_sample_shuffle_multiple_trials(tuple_list, sort_key, reverse, sample_size, trials):\n",
    "    \"\"\"\n",
    "    Example usage\n",
    "    \n",
    "    tuple_list = [\n",
    "        (1, \"http://example.com\", \"classA\", '10.00%', '15.00%'),\n",
    "        (2, \"http://example.org\", \"classB\", '5.50%', '20.00%'),\n",
    "        (3, \"http://example.net\", \"classC\", '8.75%', '12.00%'),\n",
    "    Outputs = sort_once_sample_shuffle_multiple_trials(tuple_list, 'win_rate', True, 2, 3)\n",
    "\n",
    "    Outputs: \n",
    "        [\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (1, 'http://example.com', 'classA', '10.00%', '15.00%')]),\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (1, 'http://example.com', 'classA', '10.00%', '15.00%')]),\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (3, 'http://example.net', 'classC', '8.75%', '12.00%')\n",
    "        ]\n",
    "    \"\"\"\n",
    "    if sort_key not in {'win_rate', 'choice_rate'}:\n",
    "        raise ValueError(\"sort_key must be 'win_rate' or 'choice_rate'\")\n",
    "\n",
    "    if sample_size < 1 or sample_size > len(tuple_list):\n",
    "        raise ValueError(\"sample_size must be between 1 and the length of tuple_list\")\n",
    "\n",
    "    # Function to convert percentage string to float\n",
    "    def convert_to_float(percentage_str):\n",
    "        return float(percentage_str.rstrip('%'))\n",
    "\n",
    "    # Determine the index for win_rate or choice_rate in the tuple\n",
    "    index = 3 if sort_key == 'win_rate' else 4\n",
    "\n",
    "    # Sort the list of tuples based on the specified index\n",
    "    sorted_list = sorted(tuple_list, key=lambda x: convert_to_float(x[index]), reverse=reverse)\n",
    "\n",
    "    # Create bins from the sorted list\n",
    "    bins = create_bins(sorted_list, sample_size)\n",
    "\n",
    "    results = []\n",
    "    for _ in range(trials):\n",
    "        # Sample from the bins for each trial\n",
    "        sample = sample_from_bins(bins)\n",
    "        shuffled_sample, original_indices = shuffle_samples_with_indices(sample)\n",
    "        results.append((shuffled_sample, original_indices))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Loading and Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "sampled_results = sort_once_sample_shuffle_multiple_trials(datasets[DATASET_NAME], sort_key=SORT_KEY, reverse=True, \n",
    "                                                       sample_size=SAMPLE_SIZE, trials=NUM_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Step 0: Initializing an OpenFlamingo model & Download pretrained weight\n",
    "'''\n",
    "from open_flamingo import create_model_and_transforms\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "if MODEL_SIZE == '3B':\n",
    "    model, image_processor, tokenizer = create_model_and_transforms(\n",
    "        clip_vision_encoder_path=\"ViT-L-14\",\n",
    "        clip_vision_encoder_pretrained=\"openai\",\n",
    "        lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "        tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "        cross_attn_every_n_layers=1,\n",
    "        cache_dir=\"PATH/TO/CACHE/DIR\"  # Defaults to ~/.cache\n",
    "    )\n",
    "\n",
    "    # grab model checkpoint from huggingface hub\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    import torch\n",
    "\n",
    "    checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "elif MODEL_SIZE == '9B':\n",
    "    model, image_processor, tokenizer = create_model_and_transforms(\n",
    "        clip_vision_encoder_path=\"ViT-L-14\",\n",
    "        clip_vision_encoder_pretrained=\"openai\",\n",
    "        lang_encoder_path=\"anas-awadalla/mpt-7b\",\n",
    "        tokenizer_path=\"anas-awadalla/mpt-7b\",\n",
    "        cross_attn_every_n_layers=4\n",
    "    )\n",
    "\n",
    "    checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "    model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Load images\n",
    "\"\"\"\n",
    "demo_image_one = Image.open(\n",
    "    requests.get(\n",
    "        'https://img.piku.co.kr/w/uploads/721FCA/2c5e54bed48887def8cf122705cf5455.jpg', stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "demo_image_two = Image.open(\n",
    "    requests.get(\n",
    "        'https://img.piku.co.kr/w/uploads/721FCA/08055401abb0f348ecd2fae8bd3e02b3.jpg',\n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "query_image = Image.open(\n",
    "    requests.get(\n",
    "        'https://img.piku.co.kr/w/uploads/721FCA/0270ada34d53b2b76139bc0b2131aa25.jpg', \n",
    "        stream=True\n",
    "    ).raw\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open('/home/miso/agi/context_samples.json')\n",
    "context_samples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 2: Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "\n",
    "vision_context = [\n",
    "\timage_processor(\n",
    "\t\tImage.open(requests.get(sample[1], stream=True).raw)\n",
    "\t) for sample in context_samples[1] #sampled_results[0][1]\n",
    "]\n",
    "\n",
    "vision_context = torch.stack(vision_context, dim=0)\n",
    "vision_context = vision_context[:3].unsqueeze(1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 3, 224, 224])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_context.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_query = [\n",
    "\timage_processor(\n",
    "\t\tImage.open(requests.get(sample[1], stream=True).raw)\n",
    "\t) for sample in sampled_results[3][1]\n",
    "]\n",
    "# vision_query = vision_query[0].unsqueeze(0).unsqueeze(1).unsqueeze(2)\n",
    "vision_query = torch.stack(vision_query, dim=0).unsqueeze(1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['With strong facial features and sharp eyes, he is very attractive.',\n",
       " 'With a baby face and captivating eyes, he is quite attractive.',\n",
       " 'With a warm and masculine appearance, he is slightly attractive.',\n",
       " 'With a charismatic gaze and fox-like features, he is moderately attractive.',\n",
       " 'With a versatile and attractive appearance, he is extremely attractive.',\n",
       " 'With bold eyebrows and heart-shaped lips, he is not attractive.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_prompt = [\n",
    "\t\"With strong facial features and sharp eyes, he is \",\n",
    "\t\"With a baby face and captivating eyes, he is \",\n",
    "\t\"With a warm and masculine appearance, he is \",\n",
    "\t\"With a charismatic gaze and fox-like features, he is \",\n",
    "\t\"With a versatile and attractive appearance, he is \",\n",
    "\t\"With bold eyebrows and heart-shaped lips, he is \"\n",
    "]\n",
    "\n",
    "degree_of_attractiveness = {\n",
    "\t0: \"extremely attractive\",\n",
    "\t1: \"very attractive\",\n",
    "\t2: \"quite attractive\",\n",
    "\t3: \"moderately attractive\",\n",
    "\t4: \"slightly attractive\",\n",
    "\t5: \"not attractive\",\n",
    "\t}\n",
    "\n",
    "# score = sampled_results[0][0]\n",
    "score = samples[0]\n",
    "\n",
    "context_prompt = [context + degree_of_attractiveness[score[i]] + \".\" for i, context in enumerate(context_prompt)]\n",
    "\n",
    "context_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image>With strong facial features and sharp eyes, he is very attractive.<|endofchunk|><image>With a baby face and captivating eyes, he is quite attractive.<|endofchunk|><image>With a warm and masculine appearance, he is slightly attractive.<|endofchunk|><image>With'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_context = \"\"\n",
    "for i, context in enumerate(context_prompt[:3]):\n",
    "\tif i == 0:\n",
    "\t\tfull_context = \"<image>\" + context\n",
    "\telse:\n",
    "\t\tfull_context += \"<|endofchunk|><image>\" + context\n",
    "query_text = \"<|endofchunk|><image>With\"\n",
    "\n",
    "full_context + query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miso/.conda/envs/openflamingo/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n",
      "2023-12-05 09:22:44.253629: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 09:22:44.897230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>With strong facial features and sharp eyes, he is very attractive.<|endofchunk|><image>With a baby face and captivating eyes, he is quite attractive.<|endofchunk|><image>With a warm and masculine appearance, he is slightly attractive.<|endofchunk|><image>With a baby face and sharp eyes, he is very attractive.<|endofchunk|>\n",
      "Trial #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>With strong facial features and sharp eyes, he is very attractive.<|endofchunk|><image>With a baby face and captivating eyes, he is quite attractive.<|endofchunk|><image>With a warm and masculine appearance, he is slightly attractive.<|endofchunk|><image>With a baby face and sharp eyes, he is very attractive.<|endofchunk|>\n",
      "Trial #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>With strong facial features and sharp eyes, he is very attractive.<|endofchunk|><image>With a baby face and captivating eyes, he is quite attractive.<|endofchunk|><image>With a warm and masculine appearance, he is slightly attractive.<|endofchunk|><image>With a baby face and sharp eyes, he is very attractive.<|endofchunk|>\n",
      "Trial #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>With strong facial features and sharp eyes, he is very attractive.<|endofchunk|><image>With a baby face and captivating eyes, he is quite attractive.<|endofchunk|><image>With a warm and masculine appearance, he is slightly attractive.<|endofchunk|><image>With a baby face and sharp eyes, he is very attractive.<|endofchunk|>\n",
      "Trial #4\n",
      "Generated text:  <image>With strong facial features and sharp eyes, he is very attractive.<|endofchunk|><image>With a baby face and captivating eyes, he is quite attractive.<|endofchunk|><image>With a warm and masculine appearance, he is slightly attractive.<|endofchunk|><image>With a baby face and captivating eyes, he is very attractive.<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "for t in range(NUM_TRIALS):\n",
    "    print(f\"Trial #{t}\")\n",
    "    vision_x = torch.cat((vision_context, vision_query[:,t,:].unsqueeze(1)), dim=1)\n",
    "\n",
    "    tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "    lang_x = tokenizer(\n",
    "        # [\"<image>He has expressive eyes, effortlessly crafting a heart shape with his hands and emanating a charming appeal.<|endofchunk|><image>He has fair skin and sharp, elongated eyes<image>He has\"],\n",
    "        [full_context + query_text],\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Step 4: Generate text\n",
    "    \"\"\"\n",
    "    generated_text = model.generate(\n",
    "        vision_x=vision_x.to('cuda'),\n",
    "        # vision_x = torch.randn(vision_x.shape).to('cuda'),\n",
    "        lang_x=lang_x[\"input_ids\"].to('cuda'),\n",
    "        attention_mask=lang_x[\"attention_mask\"].to('cuda'),\n",
    "        max_new_tokens=20,\n",
    "        num_beams=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>With expressive eyes, effortlessly crafting a heart shape with his hands and emanating a charming appeal, he is very attractive.<|endofchunk|><image>With fair skin and sharp, elongated eyes, he is extremely attractive.<image>With sharp eyes and sharp features, he is very attractive.<|endofchunk|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "lang_x = tokenizer(\n",
    "    [\"<image>With expressive eyes, effortlessly crafting a heart shape with his hands and emanating a charming appeal, he is very attractive.<|endofchunk|><image>With fair skin and sharp, elongated eyes, he is extremely attractive.<image>With\"],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_x.to('cuda'),\n",
    "    lang_x=lang_x[\"input_ids\"].to('cuda'),\n",
    "    attention_mask=lang_x[\"attention_mask\"].to('cuda'),\n",
    "    max_new_tokens=20,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openflamingo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
