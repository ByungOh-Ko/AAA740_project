{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 4\n",
    "NUM_TRIALS = 5\n",
    "DATASET_NAME = 'idoll_man'\n",
    "SORT_KEY = 'choice_rate'\n",
    "\n",
    "MODEL_SIZE = '9B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "root_dir = './'\n",
    "datasets = {}\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset00.pkl'), 'rb') as f:\n",
    "    idoll_man = pickle.load(f)\n",
    "datasets['idoll_man'] = idoll_man\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset01.pkl'), 'rb') as f:\n",
    "    idoll_woman = pickle.load(f)\n",
    "datasets['idoll_woman'] = idoll_woman\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset02.pkl'), 'rb') as f:\n",
    "    paintings = pickle.load(f)\n",
    "datasets['paintings'] = paintings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def create_bins(sorted_list, sample_size):\n",
    "    # Create bins from the sorted list\n",
    "    bin_size = max(1, math.ceil(len(sorted_list) / sample_size))\n",
    "    bins = [sorted_list[i:i + bin_size] for i in range(0, len(sorted_list), bin_size)]\n",
    "    return bins\n",
    "\n",
    "def sample_from_bins(bins):\n",
    "    # Randomly select one element from each bin\n",
    "    return [random.choice(bin) for bin in bins if bin]\n",
    "\n",
    "def shuffle_samples_with_indices(samples):\n",
    "    indexed_samples = list(enumerate(samples))\n",
    "    random.shuffle(indexed_samples)\n",
    "    shuffled_samples, indices = zip(*indexed_samples)\n",
    "    return list(shuffled_samples), list(indices)\n",
    "\n",
    "def sort_once_sample_shuffle_multiple_trials(tuple_list, sort_key, reverse, sample_size, trials):\n",
    "    \"\"\"\n",
    "    Example usage\n",
    "    \n",
    "    tuple_list = [\n",
    "        (1, \"http://example.com\", \"classA\", '10.00%', '15.00%'),\n",
    "        (2, \"http://example.org\", \"classB\", '5.50%', '20.00%'),\n",
    "        (3, \"http://example.net\", \"classC\", '8.75%', '12.00%'),\n",
    "    Outputs = sort_once_sample_shuffle_multiple_trials(tuple_list, 'win_rate', True, 2, 3)\n",
    "\n",
    "    Outputs: \n",
    "        [\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (1, 'http://example.com', 'classA', '10.00%', '15.00%')]),\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (1, 'http://example.com', 'classA', '10.00%', '15.00%')]),\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (3, 'http://example.net', 'classC', '8.75%', '12.00%')\n",
    "        ]\n",
    "    \"\"\"\n",
    "    if sort_key not in {'win_rate', 'choice_rate'}:\n",
    "        raise ValueError(\"sort_key must be 'win_rate' or 'choice_rate'\")\n",
    "\n",
    "    if sample_size < 1 or sample_size > len(tuple_list):\n",
    "        raise ValueError(\"sample_size must be between 1 and the length of tuple_list\")\n",
    "\n",
    "    # Function to convert percentage string to float\n",
    "    def convert_to_float(percentage_str):\n",
    "        return float(percentage_str.rstrip('%'))\n",
    "\n",
    "    # Determine the index for win_rate or choice_rate in the tuple\n",
    "    index = 3 if sort_key == 'win_rate' else 4\n",
    "\n",
    "    # Sort the list of tuples based on the specified index\n",
    "    sorted_list = sorted(tuple_list, key=lambda x: convert_to_float(x[index]), reverse=reverse)\n",
    "\n",
    "    # Create bins from the sorted list\n",
    "    bins = create_bins(sorted_list, sample_size)\n",
    "\n",
    "    results = []\n",
    "    for _ in range(trials):\n",
    "        # Sample from the bins for each trial\n",
    "        sample = sample_from_bins(bins)\n",
    "        shuffled_sample, original_indices = shuffle_samples_with_indices(sample)\n",
    "        results.append((shuffled_sample, original_indices))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccc872d5297482ab3e9aee49f5b34e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flamingo model initialized with 1384781840 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "if MODEL_SIZE == '3B':\n",
    "    model, image_processor, tokenizer = create_model_and_transforms(\n",
    "        clip_vision_encoder_path=\"ViT-L-14\",\n",
    "        clip_vision_encoder_pretrained=\"openai\",\n",
    "        lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "        tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "        cross_attn_every_n_layers=1\n",
    "    )\n",
    "\n",
    "    checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "elif MODEL_SIZE == '9B':\n",
    "    model, image_processor, tokenizer = create_model_and_transforms(\n",
    "        clip_vision_encoder_path=\"ViT-L-14\",\n",
    "        clip_vision_encoder_pretrained=\"openai\",\n",
    "        lang_encoder_path=\"anas-awadalla/mpt-7b\",\n",
    "        tokenizer_path=\"anas-awadalla/mpt-7b\",\n",
    "        cross_attn_every_n_layers=4\n",
    "    )\n",
    "\n",
    "    checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Loading and Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "sampled_results = sort_once_sample_shuffle_multiple_trials(datasets[DATASET_NAME], sort_key=SORT_KEY, reverse=True, \n",
    "                                                       sample_size=SAMPLE_SIZE, trials=NUM_TRIALS)\n",
    "\n",
    "vision_context = [\n",
    "    image_processor(Image.open(\n",
    "        requests.get(\n",
    "            sample[1], stream=True\n",
    "        ).raw\n",
    "    )).unsqueeze(0)\n",
    "    for sample in sampled_results[0][1]\n",
    "]\n",
    "\n",
    "vision_context = torch.cat(vision_context, dim=0)\n",
    "vision_context = vision_context.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "# vision_queries = [\n",
    "#     torch.stack([image_processor(Image.open(requests.get(sample[1], stream=True).raw)) \n",
    "#                 for sample in sampled_results[i + 1][1]], dim=0)\n",
    "#                     for i in range(NUM_TRIALS - 1)]\n",
    "# vision_queries = torch.stack(vision_queries, dim=0)\n",
    "# vision_queries = vision_queries.unsqueeze(2)\n",
    "\n",
    "# print(\"batch_size x num_media x num_frames x channels x height x width\")\n",
    "# print(vision_context.shape, vision_queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>How much beauty score do you think this image has? Answer: 5.0<|endofchunk|><image>How much beauty score do you think this image has? Answer: 4.3<|endofchunk|><image>How much beauty score do you think this image has? Answer: 7.0<|endofchunk|>\n",
      "<image>How much beauty score do you think this image has? Answer:\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "\n",
    "# task_prompt_text = f\"Score these {SAMPLE_SIZE} images for aesthetic quality. The score cannot be exceed the number of images\"\n",
    "\n",
    "task_prompt_text = f\"How much beauty score do you think this image has? \"\n",
    "\n",
    "# context_answer_text = list(map(str, sampled_results[0][0])) # 3, 1, 0, 2\n",
    "\n",
    "beauty_points = []\n",
    "for sample in sampled_results[0][1]:\n",
    "    val = round(float(sample[4].replace('%', '')) / 10, 1)\n",
    "    beauty_points.append(str(val))\n",
    "\n",
    "# context_answer_text = list(sampled_results[0])\n",
    "\n",
    "# context_text = ''.join([\n",
    "#     \"<image>\" + task_prompt_text + f\" Answer: {context_answer_text[i]}\" + \"<|endofchunk|>\"\n",
    "#     for i in range(SAMPLE_SIZE)\n",
    "# ])\n",
    "\n",
    "context_text = ''.join([\n",
    "    \"<image>\" + task_prompt_text + \"Answer: \" + beauty_points[i] + \"<|endofchunk|>\"\n",
    "    for i in range(SAMPLE_SIZE - 1)\n",
    "])\n",
    "\n",
    "queries_text = [\n",
    "    \"<image>\" + task_prompt_text + \"Answer:\"\n",
    "]\n",
    "\n",
    "print(context_text)\n",
    "print(queries_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>How much beauty score do you think this image has? Answer: 5.0<|endofchunk|><image>How much beauty score do you think this image has? Answer: 4.3<|endofchunk|><image>How much beauty score do you think this image has? Answer: 7.0<|endofchunk|><image>How much beauty score do you think this image has? Answer: 4.8\n"
     ]
    }
   ],
   "source": [
    "lang_x=tokenizer([context_text + queries_text[0]], \n",
    "                 return_tensors=\"pt\",\n",
    "                 )\n",
    "\n",
    "generated_text = model.generate(\n",
    "    vision_x=vision_context,\n",
    "    lang_x=lang_x[\"input_ids\"],\n",
    "    attention_mask=lang_x[\"attention_mask\"],\n",
    "    max_new_tokens=3,\n",
    "    num_beams=3,\n",
    ")\n",
    "\n",
    "print(\"Generated text: \", tokenizer.decode(generated_text[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5.0', '4.3', '7.0', '2.0']\n"
     ]
    }
   ],
   "source": [
    "print(beauty_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Step 4: Generate text\n",
    "# \"\"\"\n",
    "# for q_i in range(NUM_TRIALS - 1):\n",
    "#     for n_i in range(SAMPLE_SIZE):\n",
    "#         vision_x = torch.cat((vision_context, vision_queries[[q_i]][:, [n_i]]), dim=1)\n",
    "#         lang_x = tokenizer(\n",
    "#             context_text + queries_text[n_i],\n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "#         generated_text = model.generate(\n",
    "#             vision_x=vision_context,\n",
    "#             lang_x=lang_x[\"input_ids\"],\n",
    "#             attention_mask=lang_x[\"attention_mask\"],\n",
    "#             max_new_tokens=50,\n",
    "#             num_beams=3,\n",
    "#         )\n",
    "#         print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
