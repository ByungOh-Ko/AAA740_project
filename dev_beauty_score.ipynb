{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 3\n",
    "NUM_TRIALS = 5\n",
    "DATASET_NAME = 'idoll_man'\n",
    "SORT_KEY = 'choice_rate'\n",
    "\n",
    "MODEL_SIZE = '9B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "\n",
    "root_dir = './'\n",
    "datasets = {}\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset00.pkl'), 'rb') as f:\n",
    "    idoll_man = pickle.load(f)\n",
    "datasets['idoll_man'] = idoll_man\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset01.pkl'), 'rb') as f:\n",
    "    idoll_woman = pickle.load(f)\n",
    "datasets['idoll_woman'] = idoll_woman\n",
    "\n",
    "with open(os.path.join(root_dir, 'Dataset02.pkl'), 'rb') as f:\n",
    "    paintings = pickle.load(f)\n",
    "datasets['paintings'] = paintings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def create_bins(sorted_list, sample_size):\n",
    "    # Create bins from the sorted list\n",
    "    bin_size = max(1, math.ceil(len(sorted_list) / sample_size))\n",
    "    bins = [sorted_list[i:i + bin_size] for i in range(0, len(sorted_list), bin_size)]\n",
    "    return bins\n",
    "\n",
    "def sample_from_bins(bins):\n",
    "    # Randomly select one element from each bin\n",
    "    return [random.choice(bin) for bin in bins if bin]\n",
    "\n",
    "def shuffle_samples_with_indices(samples):\n",
    "    indexed_samples = list(enumerate(samples))\n",
    "    random.shuffle(indexed_samples)\n",
    "    shuffled_samples, indices = zip(*indexed_samples)\n",
    "    return list(shuffled_samples), list(indices)\n",
    "\n",
    "def sort_once_sample_shuffle_multiple_trials(tuple_list, sort_key, reverse, sample_size, trials):\n",
    "    \"\"\"\n",
    "    Example usage\n",
    "    \n",
    "    tuple_list = [\n",
    "        (1, \"http://example.com\", \"classA\", '10.00%', '15.00%'),\n",
    "        (2, \"http://example.org\", \"classB\", '5.50%', '20.00%'),\n",
    "        (3, \"http://example.net\", \"classC\", '8.75%', '12.00%'),\n",
    "    Outputs = sort_once_sample_shuffle_multiple_trials(tuple_list, 'win_rate', True, 2, 3)\n",
    "\n",
    "    Outputs: \n",
    "        [\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (1, 'http://example.com', 'classA', '10.00%', '15.00%')]),\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (1, 'http://example.com', 'classA', '10.00%', '15.00%')]),\n",
    "            ([1, 0],\n",
    "             [(2, 'http://example.org', 'classB', '5.50%', '20.00%'),\n",
    "             (3, 'http://example.net', 'classC', '8.75%', '12.00%')\n",
    "        ]\n",
    "    \"\"\"\n",
    "    if sort_key not in {'win_rate', 'choice_rate'}:\n",
    "        raise ValueError(\"sort_key must be 'win_rate' or 'choice_rate'\")\n",
    "\n",
    "    if sample_size < 1 or sample_size > len(tuple_list):\n",
    "        raise ValueError(\"sample_size must be between 1 and the length of tuple_list\")\n",
    "\n",
    "    # Function to convert percentage string to float\n",
    "    def convert_to_float(percentage_str):\n",
    "        return float(percentage_str.rstrip('%'))\n",
    "\n",
    "    # Determine the index for win_rate or choice_rate in the tuple\n",
    "    index = 3 if sort_key == 'win_rate' else 4\n",
    "\n",
    "    # Sort the list of tuples based on the specified index\n",
    "    sorted_list = sorted(tuple_list, key=lambda x: convert_to_float(x[index]), reverse=reverse)\n",
    "\n",
    "    # Create bins from the sorted list\n",
    "    bins = create_bins(sorted_list, sample_size)\n",
    "\n",
    "    results = []\n",
    "    for _ in range(trials):\n",
    "        # Sample from the bins for each trial\n",
    "        sample = sample_from_bins(bins)\n",
    "        shuffled_sample, original_indices = shuffle_samples_with_indices(sample)\n",
    "        results.append((shuffled_sample, original_indices))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52da88ea20654068a1a25de78dd4f6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_225362/3083393514.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtokenizer_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"anas-awadalla/mpt-7b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcross_attn_every_n_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PATH/TO/CACHE/DIR\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/flamingo/lib/python3.7/site-packages/open_flamingo/src/factory.py\u001b[0m in \u001b[0;36mcreate_model_and_transforms\u001b[0;34m(clip_vision_encoder_path, clip_vision_encoder_pretrained, lang_encoder_path, tokenizer_path, cross_attn_every_n_layers, use_local_files, decoder_layers_attr_name, **flamingo_kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdecoder_layers_attr_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mdecoder_layers_attr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_decoder_layers_attr_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mlang_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_decoder_layers_attr_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_layers_attr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mlang_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/flamingo/lib/python3.7/site-packages/open_flamingo/src/factory.py\u001b[0m in \u001b[0;36m_infer_decoder_layers_attr_name\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     raise ValueError(\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;34mf\"We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: We require the attribute name for the nn.ModuleList in the decoder storing the transformer block layers. Please supply this string manually."
     ]
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "if MODEL_SIZE == '3B':\n",
    "    model, image_processor, tokenizer = create_model_and_transforms(\n",
    "        clip_vision_encoder_path=\"ViT-L-14\",\n",
    "        clip_vision_encoder_pretrained=\"openai\",\n",
    "        lang_encoder_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "        tokenizer_path=\"anas-awadalla/mpt-1b-redpajama-200b\",\n",
    "        cross_attn_every_n_layers=1\n",
    "    )\n",
    "\n",
    "    checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-3B-vitl-mpt1b\", \"checkpoint.pt\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "elif MODEL_SIZE == '9B':\n",
    "    model, image_processor, tokenizer = create_model_and_transforms(\n",
    "        clip_vision_encoder_path=\"ViT-L-14\",\n",
    "        clip_vision_encoder_pretrained=\"openai\",\n",
    "        lang_encoder_path=\"anas-awadalla/mpt-7b\",\n",
    "        tokenizer_path=\"anas-awadalla/mpt-7b\",\n",
    "        cross_attn_every_n_layers=4,\n",
    "    )\n",
    "\n",
    "    checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size x num_media x num_frames x channels x height x width\n",
      "torch.Size([1, 3, 1, 3, 224, 224]) torch.Size([4, 3, 1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "Step 1: Loading and Preprocessing images\n",
    "Details: For OpenFlamingo, we expect the image to be a torch tensor of shape \n",
    " batch_size x num_media x num_frames x channels x height x width. \n",
    " In this case batch_size = 1, num_media = 3, num_frames = 1,\n",
    " channels = 3, height = 224, width = 224.\n",
    "\"\"\"\n",
    "sampled_results = sort_once_sample_shuffle_multiple_trials(datasets[DATASET_NAME], sort_key=SORT_KEY, reverse=True, \n",
    "                                                       sample_size=SAMPLE_SIZE, trials=NUM_TRIALS)\n",
    "\n",
    "vision_context = [\n",
    "    image_processor(Image.open(\n",
    "        requests.get(\n",
    "            sample[1], stream=True\n",
    "        ).raw\n",
    "    ))\n",
    "    for sample in sampled_results[0][1]\n",
    "]\n",
    "vision_context = torch.stack(vision_context, dim=0)\n",
    "vision_context = vision_context.unsqueeze(1).unsqueeze(0)\n",
    "\n",
    "vision_queries = [\n",
    "    torch.stack([image_processor(Image.open(requests.get(sample[1], stream=True).raw)) \n",
    "                for sample in sampled_results[i + 1][1]], dim=0)\n",
    "                    for i in range(NUM_TRIALS - 1)]\n",
    "vision_queries = torch.stack(vision_queries, dim=0)\n",
    "vision_queries = vision_queries.unsqueeze(2)\n",
    "\n",
    "print(\"batch_size x num_media x num_frames x channels x height x width\")\n",
    "print(vision_context.shape, vision_queries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 1<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 0<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 2<|endofchunk|>\n",
      "<image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 3: Preprocessing text\n",
    "Details: In the text we expect an <image> special token to indicate where an image is.\n",
    " We also expect an <|endofchunk|> special token to indicate the end of the text \n",
    " portion associated with an image.\n",
    "\"\"\"\n",
    "tokenizer.padding_side = \"left\" # For generation padding tokens should be on the left\n",
    "\n",
    "# task_prompt_text = f\"These are {SAMPLE_SIZE} images. Rank these images from most to least aligned with the standard of beauty.\"\n",
    "task_prompt_text = f\"Score these {SAMPLE_SIZE} images for aesthetic quality. The score cannot be exceed the number of images\"\n",
    "\n",
    "# context_text = \"<image><|endofchunk|>\" * SAMPLE_SIZE\n",
    "# context_text = \"<image>\" * SAMPLE_SIZE\n",
    "# context_prompt_text = task_prompt_text + \" Answer: \"\n",
    "context_answer_text = list(map(str, sampled_results[0][0])) # 3, 1, 0, 2\n",
    "# context_text = context_text + context_prompt_text + context_answer_text\n",
    "context_text = ''.join([\n",
    "    \"<image>\" + task_prompt_text + f\" Answer: {context_answer_text[i]}\" + \"<|endofchunk|>\"\n",
    "    for i in range(SAMPLE_SIZE)\n",
    "])\n",
    "\n",
    "# queries_text = [\"<image><|endofchunk|>\" * SAMPLE_SIZE] * (NUM_TRIALS - 1)\n",
    "# queries_text = [\"<image>\" * SAMPLE_SIZE] * (NUM_TRIALS - 1)\n",
    "# query_prompt_text = task_prompt_text + \" Answer: \"\n",
    "# queries_text = [\n",
    "#     query_text + query_prompt_text for query_text in queries_text\n",
    "# ]\n",
    "queries_text = [\n",
    "    \"<image>\" + task_prompt_text + f\" Answer: \"\n",
    "    for i in range(SAMPLE_SIZE)\n",
    "]\n",
    "print(context_text)\n",
    "print(queries_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/byungoh/.conda/envs/pt/lib/python3.8/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 1<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 0<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 2<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 001 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 002 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 003 Score these 3 images for aesthetic quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 1<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 0<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 2<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 001 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 002 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 003 Score these 3 images for aesthetic quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 1<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 0<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 2<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 001 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 002 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 003 Score these 3 images for aesthetic quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 1<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 0<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 2<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 001 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 002 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 003 Score these 3 images for aesthetic quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 1<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 0<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 2<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 001 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 002 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 003 Score these 3 images for aesthetic quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  <image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 1<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 0<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 2<|endofchunk|><image>Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 001 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 002 Score these 3 images for aesthetic quality. The score cannot be exceed the number of images Answer: 003 Score these 3 images for aesthetic quality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 4: Generate text\n",
    "\"\"\"\n",
    "for q_i in range(NUM_TRIALS - 1):\n",
    "    for n_i in range(SAMPLE_SIZE):\n",
    "        vision_x = torch.cat((vision_context, vision_queries[[q_i]][:, [n_i]]), dim=1)\n",
    "        lang_x = tokenizer(\n",
    "            context_text + queries_text[n_i],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        generated_text = model.generate(\n",
    "            vision_x=vision_context,\n",
    "            lang_x=lang_x[\"input_ids\"],\n",
    "            attention_mask=lang_x[\"attention_mask\"],\n",
    "            max_new_tokens=50,\n",
    "            num_beams=3,\n",
    "        )\n",
    "        print(\"Generated text: \", tokenizer.decode(generated_text[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
